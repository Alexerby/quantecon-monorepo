# Quantitative Economics & ML Implementation Lab 2026
This repository contains from-scratch implementations of core machine learning algorithms, developed as part of a systematic study of statistical learning and optimization.

## üõ† Progress Tracker

### 1. Splines & Basis Models
*Foundational models using basis expansion and piecewise polynomials.*

| Topic | Key Math / Concept | Status | Implementation |
| :--- | :--- | :---: | :--- |
| **Linear Spline** | Truncated Power Basis: $(x-\xi)_+$ | ‚úÖ | `basis_models/splines.py` |
| **Cubic Spline** | $C^2$ Continuity & Smoothness | ‚úÖ | `basis_models/splines.py` |
| **Smoothing Spline** | Interpolation | ‚úÖ | `basis_models/splines.py` |
| **Natural Spline** | Boundary constraints (Linear at edges) | üèóÔ∏è | |
| **B-splines** | Local support & Numerical stability | ‚¨ú | |
| **GAMs** | Additive components: $\sum f_j(x_j)$ | ‚¨ú | |

### 2. Ensemble Methods
*Methods for combining weak learners to reduce bias and variance.*

| Topic | Key Math / Concept | Status | Implementation |
| :--- | :--- | :---: | :--- |
| **Decision Trees** | Gini Impurity / Information Gain | ‚úÖ | `tree/decision_trees.py` |
| **Bagging** | Variance reduction / Bootstrapping | ‚úÖ | `ensembles/bagging.py` |
| **AdaBoost** | Weighted Error Minimization | ‚úÖ | `ensembles/adaboost.py` |
| **Gradient Boosting**| Residual fitting via Gradient Descent | ‚úÖ | `ensembles/gbm.py` |
| **XGBoost** | 2nd-order Taylor expansion | ‚¨ú | |
| **LightGBM** | Histogram-based growth | ‚¨ú | |
| **CatBoost** | Ordered Boosting | ‚¨ú | |

### 3. Support Vector Machines (SVM)
*Maximum margin classifiers and kernel tricks.*

| Topic | Key Math / Concept | Status | Implementation |
| :--- | :--- | :---: | :--- |
| **Hard/Soft Margin** | Hinge Loss & Slacks ($\xi$) | ‚¨ú | |
| **Linear Kernel** | Hyperplane: $w^T x + b$ | ‚¨ú | |
| **Polynomial Kernel** | Manual Feature Expansion | ‚¨ú | |
| **RBF/Gaussian** | Infinite Dimensional Mapping | ‚¨ú | |

### 4. Probabilistic & Generative Models
*Bayesian approaches and density estimation.*

| Topic | Key Math / Concept | Status | Implementation |
| :--- | :--- | :---: | :--- |
| **Multivariate Gaussian**| Covariance Matrix $\Sigma$ | ‚¨ú | |
| **Gaussian Processes**| Kernel/Covariance Functions | ‚¨ú | |
| **GMM** | Expectation-Maximization (EM) | ‚¨ú | |
| **LDA vs QDA** | Decision Boundary Geometry | ‚¨ú | |

### 5. Unsupervised Learning & Dimensionality Reduction
*Finding structure in unlabeled data.*

| Topic | Key Math / Concept | Status | Implementation |
| :--- | :--- | :---: | :--- |
| **k-means** | Centroid minimization | ‚¨ú | |
| **Hierarchical** | Linkage types (Ward/Complete) | ‚¨ú | |
| **PCA** | Eigenvalues / Variance Maximization | ‚¨ú | |
| **t-SNE** | KL-Divergence / Manifold mapping | ‚¨ú | |
| **Autoencoders** | Bottleneck Reconstruction Loss | ‚úÖ | `nn/models/feed_forward.py` |
